{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab53582",
   "metadata": {},
   "source": [
    "### Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4554ee",
   "metadata": {},
   "source": [
    "Grid Search CV (Cross-Validation) is a hyperparameter tuning technique used in machine learning to systematically search for the optimal set of hyperparameters for a given model. Hyperparameters are the configuration settings of a model that are not learned from the data but are set prior to the training process. Examples include learning rates, regularization strengths, and kernel types in various machine learning algorithms.\n",
    "\n",
    "The purpose of Grid Search CV is to automate the process of hyperparameter tuning by searching through a predefined grid of hyperparameter values and selecting the combination that results in the best model performance. This is done in conjunction with cross-validation to ensure a robust evaluation of the model across different subsets of the training data.\n",
    "\n",
    "Here's how Grid Search CV works:\n",
    "\n",
    "1. Define a Hyperparameter Grid:\n",
    "\n",
    "Specify a grid of hyperparameter values to explore. For each hyperparameter, provide a set of possible values that you want to evaluate. The Cartesian product of these sets creates a grid of hyperparameter combinations.\n",
    "\n",
    "2. Model Selection:\n",
    "\n",
    "Choose a machine learning model and instantiate it with default hyperparameter values.\n",
    "\n",
    "3. Grid Search:\n",
    "\n",
    "Use the grid of hyperparameter values and the model to perform a search across the hyperparameter space. This involves training and evaluating the model for each combination of hyperparameters using cross-validation.\n",
    "\n",
    "4. Fit Grid Search to Data:\n",
    "\n",
    "Fit the GridSearchCV object to the training data. This process involves training and evaluating the model for each hyperparameter combination while using cross-validation.\n",
    "\n",
    "5. Retrieve Best Hyperparameters:\n",
    "\n",
    "After the grid search is complete, the best hyperparameters are determined based on the performance metric (e.g., accuracy, F1 score) specified during the grid search setup\n",
    "\n",
    "6. Evaluate on Test Set:\n",
    "\n",
    "Finally, use the best hyperparameters to train the model on the entire training set and evaluate its performance on a separate test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e1c23f",
   "metadata": {},
   "source": [
    "### Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1761e03",
   "metadata": {},
   "source": [
    "Both Grid Search CV (Cross-Validation) and Randomized Search CV are techniques used for hyperparameter tuning in machine learning, where the goal is to find the best set of hyperparameters for a given model. The primary difference between them lies in how they explore the hyperparameter space.\n",
    "\n",
    "#### Grid Search CV:\n",
    "\n",
    "Exploration Method: Grid search systematically explores a predefined set of hyperparameter combinations.\n",
    "Process: It creates a grid of all possible hyperparameter combinations, then performs cross-validation for each combination and selects the one that yields the best performance.\n",
    "Pros:\n",
    "Exhaustive search ensures that all possible combinations are considered.\n",
    "It is suitable when the hyperparameter space is relatively small.\n",
    "Cons:\n",
    "Computationally expensive, especially when the hyperparameter space is large.\n",
    "\n",
    "2. Randomized Search CV:\n",
    "\n",
    "Exploration Method: Randomized search explores a random subset of the hyperparameter space.\n",
    "Process: It randomly samples a specified number of hyperparameter combinations from the predefined space and evaluates their performance using cross-validation.\n",
    "Pros:\n",
    "Computationally less expensive than grid search, making it suitable for large hyperparameter spaces.\n",
    "Effective in finding good hyperparameter combinations even with limited search.\n",
    "Cons:\n",
    "There's a chance it might miss optimal hyperparameter combinations explored in grid search.\n",
    "\n",
    "#### Choosing Between Grid Search CV and Randomized Search CV:\n",
    "\n",
    "##### Grid Search CV:\n",
    "\n",
    "Choose grid search when the hyperparameter space is relatively small, and you want to perform an exhaustive search.\n",
    "It's suitable when computational resources are not a significant constraint.\n",
    "\n",
    "##### Randomized Search CV:\n",
    "\n",
    "Choose randomized search when the hyperparameter space is large, and exploring all combinations is computationally expensive.\n",
    "It's more efficient when you have limited computational resources or time.\n",
    "\n",
    "##### Considerations:\n",
    "\n",
    "In some cases, starting with a randomized search to get a sense of the hyperparameter space and then refining the search using grid search can be a practical approach.\n",
    "Randomized search is beneficial when you want to quickly identify a promising region of the hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef6836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6173db4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ffa2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e7224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418575b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bf261de",
   "metadata": {},
   "source": [
    "### Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85041136",
   "metadata": {},
   "source": [
    "Data leakage in machine learning refers to the situation where information from the training data is inadvertently used in the model development process in a way that could not be replicated when the model encounters new, unseen data. This can lead to overly optimistic performance estimates during model training and validation, but the model may fail to generalize well to real-world scenarios. Data leakage is a problem because it can result in models that perform poorly on new data, leading to inaccurate predictions in practical applications.\n",
    "\n",
    "#####  Example of Data Leakage:\n",
    "\n",
    "Let's consider an example to illustrate data leakage:\n",
    "\n",
    "Suppose you are building a credit scoring model to predict whether a loan applicant is likely to default on their loan. The dataset contains information about the applicants, including their income, credit history, and whether they have previously defaulted on loans.\n",
    "\n",
    "Now, imagine that the dataset also includes a variable indicating whether a person has been late on a payment in the last month. During the data preprocessing step, you decide to impute missing values in the \"income\" feature by taking the mean income of all applicants. However, you mistakenly impute missing income values using the mean income of the entire dataset, including both approved and rejected loan applicants.\n",
    "\n",
    "In this case, you have introduced data leakage because the imputation of missing income values is based on information that would not be available at the time of making a credit decision. The mean income computed from the entire dataset includes information from both approved and rejected loan applicants. This creates a scenario where the model is trained on information that reflects the target variable (loan approval) and, as a result, may perform unrealistically well on the training data.\n",
    "\n",
    "The problem arises when the model is deployed to make predictions on new loan applications. The mean income computed from the entire dataset during training might not be representative of the information available for a new applicant, leading to inaccurate predictions and potentially higher default rates than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5b6980",
   "metadata": {},
   "source": [
    "### Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2518c9",
   "metadata": {},
   "source": [
    "Data leakage in machine learning occurs when information from the training data is used inappropriately during the model training process, leading to overly optimistic performance estimates. This can result in a model that performs well on training and validation data but fails to generalize to new, unseen data. To prevent data leakage, consider the following best practices:\n",
    "\n",
    "1. Separate Training and Testing Sets:\n",
    "\n",
    "Split your dataset into distinct training and testing sets. The training set is used exclusively for training the model, while the testing set is reserved for evaluating its performance on unseen data. This helps assess the model's ability to generalize.\n",
    "\n",
    "2. Avoid Using Future Information:\n",
    "\n",
    "Ensure that no information from the future is used in the training process. Features, labels, or any other data that wouldn't be available at the time of prediction should not be used.\n",
    "\n",
    "3. Cross-Validation:\n",
    "\n",
    "If you are using cross-validation, be cautious not to leak information across folds. Each training fold should only use information available up to the corresponding validation fold. This helps in obtaining a more robust estimate of the model's performance.\n",
    "\n",
    "4. Feature Engineering Awareness:\n",
    "\n",
    "Be mindful of the features used in the model. Ensure that feature engineering steps do not inadvertently include information that the model would not have access to at prediction time. For instance, computing aggregates or statistics over the entire dataset before splitting into training and testing sets could introduce leakage.\n",
    "\n",
    "5. Time Series Considerations:\n",
    "\n",
    "If working with time series data, maintain the temporal order when splitting the data. The training set should precede the validation and test sets in time. This is crucial to simulate real-world scenarios where the model has to make predictions for the future based on past information.\n",
    "\n",
    "6. Handle Categorical Variables Properly:\n",
    "\n",
    "When dealing with categorical variables, make sure to one-hot-encode them or use other encoding techniques after splitting the data. If you encode categorical variables using information from the entire dataset before splitting, you risk introducing leakage.\n",
    "\n",
    "7. Feature Scaling and Transformation:\n",
    "\n",
    "Apply feature scaling and transformations separately on the training and testing sets. This ensures that the scaling parameters are learned from the training data only and are not influenced by information in the testing set.\n",
    "\n",
    "8. Be Wary of Data Cleaning Techniques:\n",
    "\n",
    "Avoid data cleaning or imputation strategies that leverage information from the entire dataset. Imputing missing values, for example, should be based on the training set only.\n",
    "\n",
    "9. Use Pipelines:\n",
    "\n",
    "Implementing a data processing pipeline can help encapsulate all the preprocessing steps. This ensures consistency between training and testing sets and reduces the risk of unintentional leakage.\n",
    "\n",
    "10. Regularly Check for Leakage:\n",
    "\n",
    "Regularly review your code and model development process to check for potential sources of data leakage. Be skeptical of overly optimistic performance estimates and investigate unexpected spikes in model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e039673c",
   "metadata": {},
   "source": [
    "### Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887027bd",
   "metadata": {},
   "source": [
    "A confusion matrix is a table used in the evaluation of the performance of a classification model. It provides a comprehensive view of how well a model has performed by comparing its predictions with the actual outcomes. The matrix is especially useful when dealing with binary or multiclass classification problems.\n",
    "\n",
    "The confusion matrix consists of four main components:\n",
    "\n",
    "1. True Positives (TP): Instances that are correctly predicted as positive by the model.\n",
    "\n",
    "2. True Negatives (TN): Instances that are correctly predicted as negative by the model.\n",
    "\n",
    "3. False Positives (FP): Instances that are incorrectly predicted as positive by the model when they are actually negative.\n",
    "\n",
    "4. False Negatives (FN): Instances that are incorrectly predicted as negative by the model when they are actually positive\n",
    "\n",
    "From the confusion matrix, several performance metrics can be derived to evaluate the model's effectiveness, including:\n",
    "\n",
    "1. Accuracy: The proportion of correctly classified instances among the total instances. Calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "\n",
    "2. Precision: The ratio of correctly predicted positive instances to the total predicted positives. Calculated as TP / (TP + FP).\n",
    "\n",
    "3. Recall (Sensitivity): The ratio of correctly predicted positive instances to all actual positives. Calculated as TP / (TP + FN).\n",
    "\n",
    "4. Specificity: The ratio of correctly predicted negative instances to all actual negatives. Calculated as TN / (TN + FP).\n",
    "\n",
    "5. F1 Score: The harmonic mean of precision and recall, providing a balance between the two metrics. Calculated as 2 * (Precision * Recall) / (Precision + Recall).\n",
    "\n",
    "Analyzing the confusion matrix helps in understanding where the model excels and where it falls short. For example:\n",
    "\n",
    "##### High values in the diagonal (TP and TN) indicate that the model is making correct predictions.\n",
    "##### False positives (FP) and false negatives (FN) can provide insights into specific types of errors made by the model.\n",
    "##### Precision and recall offer a trade-off: a model can achieve high precision at the expense of recall and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce05659",
   "metadata": {},
   "source": [
    "### Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c91332d",
   "metadata": {},
   "source": [
    "\n",
    "Precision and recall are two important metrics derived from a confusion matrix, and they provide insights into different aspects of a classification model's performance. Here's an explanation of the key differences between precision and recall:\n",
    "\n",
    "1. Precision:\n",
    "\n",
    "Definition: Precision, also known as positive predictive value, measures the accuracy of the positive predictions made by the model. It answers the question, \"Of all instances predicted as positive, how many are actually positive?\"\n",
    "Formula: Precision = TP / (TP + FP)\n",
    "Interpretation: A high precision indicates that the model is good at avoiding false positives. It is the ratio of correctly predicted positive instances (TP) to the total instances predicted as positive (TP + FP).\n",
    "\n",
    "2. Recall (Sensitivity):\n",
    "\n",
    "Definition: Recall, also known as sensitivity or true positive rate, measures the ability of the model to capture all the actual positive instances. It answers the question, \"Of all actual positive instances, how many did the model correctly predict?\"\n",
    "Formula: Recall = TP / (TP + FN)\n",
    "Interpretation: A high recall indicates that the model is good at identifying most of the positive instances. It is the ratio of correctly predicted positive instances (TP) to the total actual positive instances (TP + FN).\n",
    "\n",
    "##### Key Differences:\n",
    "\n",
    "Emphasis on Errors:\n",
    "\n",
    "Precision focuses on minimizing false positives (FP). It is concerned with the accuracy of positive predictions and avoiding the misclassification of negative instances as positive.\n",
    "Recall focuses on minimizing false negatives (FN). It is concerned with capturing as many positive instances as possible and avoiding the misclassification of positive instances as negative.\n",
    "\n",
    "Trade-off:\n",
    "\n",
    "Precision and recall often have a trade-off. Increasing precision may decrease recall and vice versa. This trade-off is particularly important in situations where there are consequences associated with false positives and false negatives.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "Precision is crucial in scenarios where the cost of false positives is high. For example, in spam email detection, misclassifying a legitimate email as spam (false positive) is undesirable.\n",
    "Recall is crucial in scenarios where the cost of false negatives is high. In medical diagnoses, missing a positive case (false negative) may have severe consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e752e91",
   "metadata": {},
   "source": [
    "### Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9da6cd",
   "metadata": {},
   "source": [
    "\n",
    "Interpreting a confusion matrix involves understanding the different components and analyzing the types of errors made by a classification model. Here's how you can interpret a confusion matrix:\n",
    "\n",
    "1. True Positives (TP):\n",
    "\n",
    "Interpretation: Instances correctly predicted as positive.\n",
    "Implication: The model correctly identified these instances as belonging to the positive class.\n",
    "\n",
    "2. True Negatives (TN):\n",
    "\n",
    "Interpretation: Instances correctly predicted as negative.\n",
    "Implication: The model correctly identified these instances as not belonging to the positive class.\n",
    "\n",
    "3. False Positives (FP):\n",
    "\n",
    "Interpretation: Instances incorrectly predicted as positive.\n",
    "Implication: The model mistakenly classified these instances as belonging to the positive class when they do not.\n",
    "\n",
    "4. False Negatives (FN):\n",
    "\n",
    "Interpretation: Instances incorrectly predicted as negative.\n",
    "Implication: The model mistakenly classified these instances as not belonging to the positive class when they do.\n",
    "\n",
    "Once you understand these components, you can derive several key insights:\n",
    "\n",
    "##### Accuracy: Overall correctness of the model. Calculated as (TP + TN) / Total.\n",
    "\n",
    "##### Precision: Proportion of instances predicted as positive that are actually positive. Calculated as TP / (TP + FP).\n",
    "\n",
    "##### Recall (Sensitivity): Proportion of actual positives that were correctly predicted. Calculated as TP / (TP + FN).\n",
    "\n",
    "##### Specificity: Proportion of actual negatives that were correctly predicted. Calculated as TN / (TN + FP).\n",
    "\n",
    "##### False Positive Rate (FPR): Proportion of actual negatives incorrectly predicted as positive. Calculated as FP / (TN + FP).\n",
    "\n",
    "##### False Negative Rate (FNR): Proportion of actual positives incorrectly predicted as negative. Calculated as FN / (TP + FN).\n",
    "\n",
    "Analyzing these metrics can help you understand the strengths and weaknesses of your model. For example:\n",
    "\n",
    "##### High Precision: The model is good at avoiding false positives.\n",
    "##### High Recall: The model is good at capturing most of the positive instances.\n",
    "##### Trade-off: There is often a trade-off between precision and recall. Improving one may come at the expense of the other.\n",
    "It's essential to consider the context of the problem and the consequences of false positives and false negatives when interpreting a confusion matrix. For instance, in a medical diagnosis scenario, false negatives (missed positive cases) might be more critical than false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512d972",
   "metadata": {},
   "source": [
    "### Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3290c9f0",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is often used to evaluate the performance of a classification algorithm. It provides a summary of the predictions made by a model compared to the actual labels of the data. From a confusion matrix, several metrics can be derived, including:\n",
    "\n",
    "1. True Positive (TP):\n",
    "\n",
    "Definition: The number of instances correctly predicted as positive.\n",
    "Calculation: The value in the top-left cell of the confusion matrix.\n",
    "\n",
    "2. True Negative (TN):\n",
    "\n",
    "Definition: The number of instances correctly predicted as negative.\n",
    "Calculation: The value in the bottom-right cell of the confusion matrix.\n",
    "\n",
    "3. False Positive (FP) or Type I Error:\n",
    "\n",
    "Definition: The number of instances incorrectly predicted as positive when they are actually negative.\n",
    "Calculation: The value in the top-right cell of the confusion matrix.\n",
    "\n",
    "4. False Negative (FN) or Type II Error:\n",
    "\n",
    "Definition: The number of instances incorrectly predicted as negative when they are actually positive.\n",
    "Calculation: The value in the bottom-left cell of the confusion matrix.\n",
    "\n",
    "Using these basic elements, several performance metrics can be calculated:\n",
    "\n",
    "1. Accuracy:\n",
    "\n",
    "Definition: The ratio of correctly predicted instances to the total instances.\n",
    "Calculation: (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "2. Precision (Positive Predictive Value):\n",
    "\n",
    "Definition: The ratio of correctly predicted positive observations to the total predicted positives.\n",
    "Calculation: TP / (TP + FP)\n",
    "\n",
    "3. Recall (Sensitivity, True Positive Rate):\n",
    "\n",
    "Definition: The ratio of correctly predicted positive observations to all actual positives.\n",
    "Calculation: TP / (TP + FN)\n",
    "\n",
    "4. Specificity (True Negative Rate):\n",
    "\n",
    "Definition: The ratio of correctly predicted negative observations to all actual negatives.\n",
    "Calculation: TN / (TN + FP)\n",
    "\n",
    "5. F1 Score:\n",
    "\n",
    "Definition: The harmonic mean of precision and recall, providing a balance between the two metrics.\n",
    "Calculation: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "6. False Positive Rate (FPR):\n",
    "\n",
    "Definition: The ratio of incorrectly predicted positives to all actual negatives.\n",
    "Calculation: FP / (TN + FP)\n",
    "\n",
    "1. False Negative Rate (FNR):\n",
    "Definition: The ratio of incorrectly predicted negatives to all actual positives.\n",
    "Calculation: FN / (TP + FN)\n",
    "\n",
    "These metrics help in assessing different aspects of a classification model's performance, such as its ability to correctly identify positive instances (sensitivity/recall) or avoid misclassifying negative instances (specificity). The choice of which metric to emphasize depends on the specific goals and requirements of the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127c423",
   "metadata": {},
   "source": [
    "### Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499175ab",
   "metadata": {},
   "source": [
    "The relationship between the accuracy of a model and the values in its confusion matrix is fundamental to understanding the overall performance of the model. The accuracy is a summary statistic derived from the confusion matrix and provides a measure of the model's correctness across all classes. Here's how accuracy relates to the confusion matrix:\n",
    "\n",
    "### Accuracy Formula:\n",
    "Accuracy = correct predictions(True Positive+ True Negative)/ Total Predictions (Sum of all elements in the confusion matrix)\n",
    "\n",
    "### Components in the Confusion Matrix:\n",
    "\n",
    "True Positives (TP): Instances correctly predicted as positive.\n",
    "True Negatives (TN): Instances correctly predicted as negative.\n",
    "False Positives (FP): Instances wrongly predicted as positive.\n",
    "False Negatives (FN): Instances wrongly predicted as negative.\n",
    "Accuracy in Terms of Confusion Matrix Components:\n",
    "\n",
    "Accuracy= (TP + TN)/(TP + TN + FP + FN)\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "Accuracy represents the proportion of correct predictions (both positive and negative) among all predictions made by the model.\n",
    "It reflects the ability of the model to make correct classifications across all classes.\n",
    "While accuracy is a commonly used metric, it may not provide a complete picture in cases of imbalanced datasets, where one class significantly outweighs the others.\n",
    "\n",
    "### Considerations:\n",
    "Accuracy is influenced by the distribution of instances across classes. If there is a class imbalance, high accuracy may not necessarily indicate good performance for minority classes.\n",
    "For a balanced dataset, high accuracy suggests that the model is performing well in terms of overall correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f262d",
   "metadata": {},
   "source": [
    "### Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d0077d",
   "metadata": {},
   "source": [
    "\n",
    "Using a confusion matrix is a valuable approach to identify potential biases or limitations in a machine learning model, especially in the context of classification tasks. Here are several ways to use a confusion matrix for this purpose:\n",
    "\n",
    "1. Class Imbalance:\n",
    "\n",
    "Check for significant imbalances in the number of instances across different classes. If there is a disproportionate distribution, the model may be biased towards the majority class, leading to a potential limitation in its ability to accurately predict minority classes.\n",
    "\n",
    "2. Precision and Recall Disparities:\n",
    "\n",
    "Examine precision and recall values for each class. If there are substantial differences in precision or recall between classes, it could indicate biases. A high precision but low recall may suggest that the model is conservative in predicting positive instances, while the opposite may indicate a more liberal approach.\n",
    "\n",
    "3. False Positives and False Negatives:\n",
    "\n",
    "Analyze false positives and false negatives for each class. Identify if certain classes are more prone to specific types of errors. For example, a model predicting a certain class as positive when it is negative (false positive) may suggest biases or limitations in capturing the characteristics of that class.\n",
    "\n",
    "4. Confusion Along Sensitive Attributes:\n",
    "\n",
    "Investigate if the model's performance varies across sensitive attributes such as gender, race, or age. Biases may arise if the model exhibits different levels of accuracy or error rates for different subgroups.\n",
    "\n",
    "5. Discrepancies in Misclassifications:\n",
    "\n",
    "Assess if the model is misclassifying certain classes more frequently than others. If specific classes consistently contribute to higher misclassification rates, it might indicate challenges in capturing the underlying patterns of those classes.\n",
    "\n",
    "6. Area Under the ROC Curve (AUC-ROC):\n",
    "\n",
    "Evaluate the AUC-ROC for each class. A model with biased predictions may exhibit variations in the discrimination ability across different classes, leading to different AUC-ROC values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de4dbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
