{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6142ee7",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70695d9",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique that builds a predictive model in the form of an ensemble of weak predictive models, typically decision trees. It is used for regression problems, aiming to predict a continuous target variable. Gradient Boosting builds the model sequentially by correcting the errors made by the previous models, optimizing a specified loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6868e0e",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ff8c7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.007864677608425885\n",
      "R-squared: 0.9828172820205644\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a small dataset for regression\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gradient Boosting Regression class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        predictions = np.zeros_like(y)\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - predictions\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            update = self.learning_rate * tree.predict(X)\n",
    "            predictions += update\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for tree in self.models:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Train the Gradient Boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = gb_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8731fa",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c445e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}\n",
      "Best Mean Squared Error: 0.011806382097885416\n",
      "Best R-squared: 0.97420546091704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the Gradient Boosting regressor\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_gb_model = GradientBoostingRegressor(**best_params)\n",
    "best_gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_best = best_gb_model.predict(X_test)\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Best Mean Squared Error: {mse_best}\")\n",
    "print(f\"Best R-squared: {r2_best}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd701cd4",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa5419",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner is a model that performs slightly better than random chance on a given problem. Typically, decision trees with a shallow depth are used as weak learners in Gradient Boosting. These weak learners are also often referred to as \"base learners\" or \"base models.\" The key characteristic of a weak learner is that it should be better than random guessing, even if only marginally so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4010ab4",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133bbbcc",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to sequentially improve the performance of a model by focusing on the mistakes made by the previous models. It combines the predictions of weak learners to create a strong learner. At each iteration, a new weak learner is trained to correct the errors of the combined model. The learning process is guided by the gradient of the loss function, aiming to minimize the difference between the predicted values and the actual target values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e465cea",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103410c8",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners through an iterative process. At each iteration, a new weak learner is trained on the residuals (the differences between the actual and predicted values) of the combined model from the previous iterations. The new learner is then added to the ensemble with a certain weight, and the process repeats. The weights of the weak learners are determined based on their ability to correct the errors made by the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d17db6c",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6bbee",
   "metadata": {},
   "source": [
    "The mathematical intuition of the Gradient Boosting algorithm can be explained through the following steps:\n",
    "\n",
    "1. Initialize the model: Start with an initial prediction, often the mean of the target variable.\n",
    "\n",
    "2. Compute residuals: Calculate the residuals by subtracting the predicted values from the actual target values.\n",
    "\n",
    "3. Train a weak learner: Train a weak learner (e.g., decision tree) on the residuals. The weak learner should fit the data, focusing on the instances where the model has made errors.\n",
    "\n",
    "4. Compute the learning rate weighted update: Multiply the predictions of the weak learner by a small learning rate (a hyperparameter that controls the contribution of each weak learner). This scaled prediction is then added to the ensemble.\n",
    "\n",
    "5. Update the residuals: Subtract the scaled predictions from the residuals to create new residuals.\n",
    "\n",
    "6. Repeat: Repeat steps 3-5 for a specified number of iterations or until a convergence criterion is met.\n",
    "\n",
    "7. Combine weak learners: The final model is the sum of all the weak learners, each multiplied by its learning rate.\n",
    "\n",
    "The algorithm aims to minimize a loss function (e.g., mean squared error) by iteratively adjusting the predictions using the gradients of the loss with respect to the predictions. The process results in a powerful ensemble model that can capture complex relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefca962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda10603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e7f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293bc0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140bf1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e1aefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000ef40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b66d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350fa2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
