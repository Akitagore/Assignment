{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deff1110",
   "metadata": {},
   "source": [
    "### Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f415e67d",
   "metadata": {},
   "source": [
    "Web Scraping is the process of extracting information from websites. It involves fetching web pages and then extracting useful information from the HTML code. Web scraping is used to automate the extraction of large amounts of data from websites, which might be time-consuming or infeasible to collect manually. \n",
    "Three areas where web scraping is commonly used include:\n",
    "1. Data Mining and Analysis: Web scraping is employed to collect large datasets for analysis, helping businesses and researchers to gather insights and make informed decisions.\n",
    "\n",
    "2. Price Monitoring and Comparison: E-commerce websites often use web scraping to monitor competitors' prices, ensuring that their own prices are competitive. Consumers may also use web scraping tools to compare prices across different platforms.\n",
    "\n",
    "3. Content Aggregation: News websites and content aggregators use web scraping to gather news articles and information from various sources to provide a centralized platform for users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ed9e9",
   "metadata": {},
   "source": [
    "### Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e5fbd",
   "metadata": {},
   "source": [
    "1. Manual Copy-Pasting: The simplest form of web scraping where users manually copy-paste information from websites.\n",
    "\n",
    "2. Regular Expressions: Regex can be used to extract specific patterns from HTML content.\n",
    "\n",
    "3. HTML Parsing: Libraries like BeautifulSoup and lxml in Python are used to parse the HTML content and extract relevant information.\n",
    "\n",
    "4. APIs: Some websites provide APIs (Application Programming Interfaces) that allow users to retrieve data in a structured format without scraping the HTML.\n",
    "\n",
    "5. Headless Browsing: Automated browsers like Selenium can be used for web scraping by simulating the user's interaction with the website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f08943",
   "metadata": {},
   "source": [
    "### Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07af13",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library used for web scraping purposes to pull the data out of HTML and XML files. It provides Pythonic idioms for iterating, searching, and modifying the parse tree. Beautiful Soup transforms a complex HTML document into a tree of Python objects, such as tags, navigable strings, or comments. It is widely used for its simplicity and ease of use in extracting data from HTML or XML documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d56a02",
   "metadata": {},
   "source": [
    "### Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806c1fb",
   "metadata": {},
   "source": [
    "Flask is a lightweight web application framework for Python. In the context of a web scraping project, Flask can be used to create a web application that serves as the user interface for interacting with the scraped data. Users can input parameters, trigger scraping actions, and view the results through a web interface. Flask provides a simple way to build web applications and APIs, making it suitable for this purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055a056",
   "metadata": {},
   "source": [
    "### Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9601f6",
   "metadata": {},
   "source": [
    "The specific AWS services used in a web scraping project can vary, but common ones might include:\n",
    "1. Amazon EC2 (Elastic Compute Cloud): EC2 instances can be used to host web scraping scripts or applications. It provides scalable compute capacity in the cloud.\n",
    "\n",
    "2. Amazon S3 (Simple Storage Service): S3 can be used to store and retrieve large amounts of data, including scraped data files. It provides scalable and secure object storage.\n",
    "\n",
    "3. AWS Lambda: Lambda can be used to run code without provisioning or managing servers. It is useful for executing web scraping tasks in a serverless environment.\n",
    "\n",
    "4. Amazon RDS (Relational Database Service): RDS can be used to store structured data extracted during web scraping. It provides a managed relational database service.\n",
    "\n",
    "5. Amazon CloudWatch: CloudWatch can be used for monitoring and logging the performance of web scraping tasks. It helps in tracking the execution of scripts and handling errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
