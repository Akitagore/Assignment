{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c199fa2",
   "metadata": {},
   "source": [
    "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee9eb4",
   "metadata": {},
   "source": [
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a linear regression technique with added L1 regularization. It introduces a penalty term based on the sum of the absolute values of the coefficients, aiming to shrink some coefficients to exactly zero. This feature makes Lasso particularly effective for feature selection, as it can exclude irrelevant variables.\n",
    "\n",
    "Compared to other regression techniques like Ordinary Least Squares (OLS), Lasso differs in its ability to perform variable selection by setting some coefficients to precisely zero, promoting sparsity in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d8d98a",
   "metadata": {},
   "source": [
    "### Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42be188",
   "metadata": {},
   "source": [
    "The main advantage of Lasso Regression in feature selection is its ability to drive some coefficients to exactly zero. This results in a sparse model, effectively performing automatic feature selection. Lasso is valuable when dealing with high-dimensional datasets or when there is a belief that many features are irrelevant. It simplifies the model and enhances interpretability by excluding less important variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ded25",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846bfa44",
   "metadata": {},
   "source": [
    "The interpretation of Lasso Regression coefficients is similar to that of linear regression. Each coefficient represents the estimated change in the target variable for a one-unit change in the corresponding predictor, holding other predictors constant. However, due to the L1 regularization, some coefficients may be exactly zero, indicating excluded variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b0f96",
   "metadata": {},
   "source": [
    "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04f50e",
   "metadata": {},
   "source": [
    "The main tuning parameter in Lasso Regression is the regularization parameter (λ), controlling the strength of the regularization. As λ increases, more coefficients are driven to zero, leading to increased sparsity. The choice of λ involves a trade-off between model fit and simplicity. Cross-validation or other techniques are often used to find the optimal λ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa30b0",
   "metadata": {},
   "source": [
    "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee2bf5",
   "metadata": {},
   "source": [
    "Lasso Regression, by itself, is a linear regression technique. However, it can be extended to handle non-linear relationships by incorporating non-linear transformations of the features. For example, polynomial features or interactions can be added to capture non-linear patterns. Additionally, Lasso can be combined with non-linear models to create hybrid approaches for addressing non-linear regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76925f6",
   "metadata": {},
   "source": [
    "### Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18624bd3",
   "metadata": {},
   "source": [
    "The main difference between Ridge Regression and Lasso Regression lies in the type of regularization they apply. Ridge uses L2 regularization, adding a penalty term based on the sum of squared coefficients, while Lasso uses L1 regularization, adding a penalty term based on the sum of absolute values of coefficients. Lasso tends to produce more sparse models by driving some coefficients exactly to zero, providing automatic feature selection, whereas Ridge generally shrinks coefficients more evenly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02fe2f4",
   "metadata": {},
   "source": [
    "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e889b",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity, a situation where predictor variables are highly correlated. Lasso's L1 regularization tends to select one variable from a group of correlated variables and drive the coefficients of the others to zero. This feature is particularly beneficial in scenarios with multicollinearity, effectively performing implicit feature selection and simplifying the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5e425",
   "metadata": {},
   "source": [
    "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be801b01",
   "metadata": {},
   "source": [
    "Selecting the optimal λ in Lasso Regression typically involves techniques like cross-validation. The dataset is divided into training and validation sets, and the model is trained with different λ values. The λ that results in the best performance on the validation set, often measured by mean squared error or another appropriate metric, is chosen as the optimal value. Grid search or more advanced optimization methods can be used to systematically explore the hyperparameter space and find the best regularization strength for the specific dataset. Regularization paths, which trace the model performance across a range of \n",
    "λ values, can aid in visualizing the trade-off between model complexity and fit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
