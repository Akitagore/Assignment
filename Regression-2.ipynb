{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2403d3",
   "metadata": {},
   "source": [
    "### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317972da",
   "metadata": {},
   "source": [
    "#### R-squared (Coefficient of Determination) in Linear Regression:\n",
    "R-squared is a statistical metric that measures the proportion of the variance in the dependent variable (Y) that is explained by the independent variable(s) in a linear regression model. It provides an indication of how well the independent variable(s) explain the variability of the dependent variable. R-squared is a scale from 0 to 1, where 0 indicates that the model does not explain any variability, and 1 indicates that the model perfectly explains the variability.\n",
    "#### Calculation of R-squared:\n",
    "The formula for calculating R-squared is as follows:\n",
    "R^2 = 1-((sum of squared Residuals (SSR)/(Total Sum of Squares(SST))\n",
    "1. Sum of Squared Residuals (SSR): It represents the sum of the squared differences between the predicted (Y pred) and actual (Y actual) values.\n",
    "SSR=∑_i=1^n(Y_actual^(i)−Ypred(i))^2\n",
    "2. Total Sum of Squares (SST): It represents the sum of the squared differences between the actual values and the mean of the dependent variable (Y_mean)\n",
    "SST=∑_i=1^n(Y_actual^(i)−Y_mean)^2\n",
    "#### Interpretation of R-squared:\n",
    "1. An R-squared value close to 1 indicates that a high proportion of the variability in the dependent variable is explained by the independent variable(s).\n",
    "2. An R-squared value close to 0 suggests that the model does not explain much of the variability, and the predictions are not accurate.\n",
    "#### Consideration:\n",
    "1. R-squared is sensitive to the number of predictors in the model. Adding more predictors, even irrelevant ones, can artificially inflate R-squared.\n",
    "2. It does not indicate the correctness of the regression coefficients or the reliability of the model.\n",
    "3. A high R-squared does not imply causation; it only quantifies the proportion of variance explained.\n",
    "#### Adjusted R-squared:\n",
    "The adjusted R-squared adjusts the R-squared value based on the number of predictors in the model. It penalizes models with additional predictors that do not contribute significantly to explaining variability\n",
    "Adjusted R^2 = 1-((1-R^2).(n-1))/(n-k-1)\n",
    "where n is the number of observations and k is number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ffe60",
   "metadata": {},
   "source": [
    "### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea465f5",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modification of the regular R-squared in linear regression. It adjusts for the number of predictors, penalizing unnecessary complexity. While regular R-squared may increase with more predictors, adjusted R-squared considers this and provides a more accurate measure of a model's goodness of fit. It ranges from negative values to 1, where higher values indicate better explanatory power, considering the trade-off between model complexity and fit. Adjusted R-squared is useful for a nuanced evaluation of model performance in multiple regression scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49d748",
   "metadata": {},
   "source": [
    "### Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac04a6e",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate to use in situations where there are multiple predictors in a linear regression model. It addresses some of the limitations of the regular R-squared and is particularly useful in the following scenarios:\n",
    "1. Multiple Predictors:\n",
    "Adjusted R-squared becomes more crucial as the number of predictors (independent variables) in the model increases. It helps account for the potential inflation of regular R-squared when adding predictors that do not significantly contribute to explanatory power.\n",
    "2. Model Comparison:\n",
    "When comparing different regression models with varying numbers of predictors, adjusted R-squared provides a fairer assessment of goodness of fit. It considers the trade-off between the model's complexity and its explanatory power.\n",
    "3. Avoiding Overfitting:\n",
    "Adjusted R-squared is useful in guarding against overfitting. As more predictors are added, regular R-squared may increase even if those predictors do not genuinely improve the model's ability to explain the variability in the dependent variable. Adjusted R-squared penalizes the addition of irrelevant predictors.\n",
    "4. Interpretability:\n",
    "When the goal is to have a model that not only fits the data well but is also interpretable, adjusted R-squared provides a more balanced evaluation. It encourages simplicity in the model while considering explanatory power.\n",
    "5. Research and Publication:\n",
    "In academic research or when publishing findings, adjusted R-squared is often preferred. It offers a more nuanced and accurate assessment of model performance, which is important for drawing valid conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bc76b",
   "metadata": {},
   "source": [
    "### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c2bc0",
   "metadata": {},
   "source": [
    "MSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used metrics in the context of regression analysis to evaluate the performance of a predictive model by measuring the difference between predicted values and actual values.\n",
    "##### Mean Absolute Error (MAE):\n",
    "1. Formula : MAE = 1/n sum_(i=1)^n|yi-hat yi|\n",
    "2. MAE represents the average absolute difference between the actual and predicted values. It gives equal weight to all errors without considering their direction. MAE is less sensitive to outliers compared to MSE and RMSE.\n",
    "##### Mean Squared Error (MSE):\n",
    "1. Formula : MAE = 1/n sum_(i=1)^n(yi-hat yi)^2\n",
    "2. MSE is obtained by squaring the differences between actual and predicted values before averaging. Squaring the errors gives more weight to larger errors and penalizes the model more for larger deviations. MSE is commonly used but can be sensitive to outliers due to the squaring operation.\n",
    "##### Root Mean Squared Error (RMSE):\n",
    "1. formula : RMSE = sqrt(MSE)\n",
    "2. RMSE is the square root of MSE and is used to provide a measure of the spread of errors in the same unit as the dependent variable. Like MSE, RMSE gives more weight to larger errors, but taking the square root helps in interpreting the result on the same scale as the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6578ed",
   "metadata": {},
   "source": [
    "### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8acd2c",
   "metadata": {},
   "source": [
    "##### Advantages and Disadvantages of RMSE, MSE, and MAE in Regression Analysis:\n",
    "\n",
    "1. Mean Absolute Error (MAE):\n",
    "\n",
    "##### Advantages:\n",
    "Simple to understand and easy to compute.\n",
    "It provides a straightforward measure of the average absolute prediction error.\n",
    "Less sensitive to outliers compared to MSE and RMSE, as it does not involve squaring the errors.\n",
    "\n",
    "##### Disadvantages:\n",
    "Ignores the direction of errors, treating overestimates and underestimates equally.\n",
    "May not be suitable if the magnitude of errors is crucial, as it does not penalize large errors more heavily.\n",
    "\n",
    "2. Mean Squared Error (MSE):\n",
    "##### Advantages:\n",
    "The squaring operation gives more weight to larger errors, making it sensitive to significant deviations.\n",
    "Differentiable, making it suitable for optimization algorithms.\n",
    "\n",
    "##### Disadvantages:\n",
    "Sensitive to outliers due to the squaring operation, which can lead to a skewed evaluation if there are extreme values.\n",
    "The unit of MSE is not the same as the original data, making interpretation less intuitive.\n",
    "\n",
    "3. Root Mean Squared Error (RMSE):\n",
    "##### Advantages:\n",
    "Provides a measure of the spread of errors in the original units, making it more interpretable than MSE.\n",
    "Similar sensitivity to large errors as MSE but avoids the unit mismatch.\n",
    "##### Disadvantages:\n",
    "Sensitive to outliers due to the squared operation, though less so than MSE.\n",
    "May penalize larger errors more heavily, which might not always be desirable.\n",
    "\n",
    "##### Choosing the Right Metric:\n",
    "1. MAE is suitable when errors need to be treated equally, and the emphasis is on the magnitude of the errors rather than their direction.\n",
    "2. MSE and RMSE are appropriate when larger errors should be penalized more heavily, and the spread of errors in the original units is important for interpretation.\n",
    "3. The choice between MSE and RMSE often depends on the preference for units – RMSE provides results in the same unit as the dependent variable, while MSE does not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6fd1e5",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4607e0",
   "metadata": {},
   "source": [
    "Lasso regularization, also known as L1 regularization, is a technique used in linear regression to prevent overfitting by adding a penalty term to the cost function. The penalty is based on the absolute values of the regression coefficients. The main objective of Lasso regularization is to encourage sparsity in the model by pushing some of the coefficient values to exactly zero. This can be particularly useful when dealing with high-dimensional datasets with many features, as Lasso tends to select a subset of the most relevant features.\n",
    "##### Differences between Lasso and Ridge regularization:\n",
    "\n",
    "1. Sparsity:\n",
    "Lasso tends to produce sparse models by driving some coefficients to exactly zero. This makes Lasso useful for feature selection, as it can eliminate less relevant features from the model.\n",
    "Ridge, on the other hand, does not result in exactly zero coefficients, and it includes all features to some extent.\n",
    "\n",
    "2. Impact on Coefficients:\n",
    "Lasso has a tendency to force the coefficients of less important features to zero more aggressively, effectively excluding them from the model.\n",
    "Ridge shrinks the coefficients towards zero but does not set them exactly to zero. It mitigates multicollinearity by distributing the impact among all features.\n",
    "\n",
    "##### When to use Lasso vs. Ridge:\n",
    "1. Use Lasso:\n",
    "When dealing with a high-dimensional dataset with many features, and there's a belief that only a subset of features is truly important.\n",
    "For feature selection and when interpretability is essential.\n",
    "2. Use Ridge:\n",
    "When multicollinearity among features is a concern, as Ridge helps to distribute the impact of correlated features.\n",
    "In situations where having all features in the model is reasonable and there is no strong belief that some features are completely irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdfad9b",
   "metadata": {},
   "source": [
    "### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97debdb0",
   "metadata": {},
   "source": [
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the cost function, discouraging overly complex models with large coefficients. Overfitting occurs when a model captures noise in the training data rather than the underlying patterns, leading to poor generalization on new, unseen data. Regularization is a technique that imposes constraints on the model parameters during training, promoting simpler models and reducing the risk of overfitting.\n",
    "\n",
    "There are two common types of regularization for linear models: L1 regularization (Lasso) and L2 regularization (Ridge).\n",
    "1. L1 Regularization (Lasso):\n",
    "Adds the sum of the absolute values of the coefficients as a penalty term to the cost function.\n",
    "Encourages sparsity in the model by driving some coefficients to exactly zero, effectively performing feature selection.\n",
    "2. L2 Regularization (Ridge):\n",
    "Adds the sum of the squared values of the coefficients as a penalty term to the cost function.\n",
    "Mitigates multicollinearity among features by distributing the impact of correlated features.\n",
    "##### Illustrative Example:\n",
    "Let's consider a simple linear regression example with L1 (Lasso) regularization. The regularized linear regression cost function with L1 regularization is given by:\n",
    "##### J(θ)=MSE(θ)+α∑_(i=1)^n∣θi∣\n",
    "Here:\n",
    "##### J(θ) is the cost function.\n",
    "##### MSE(θ) is the mean squared error, measuring the model's fit to the training data.\n",
    "##### α is the regularization parameter, controlling the strength of regularization.\n",
    "##### ∑_(i=1)^n∣θi∣ is the L1 regularization term, penalizing the absolute values of the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b184d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.6584189249611411\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Lasso regularization\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lasso_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "\n",
    "# Plot the model\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=3, label='Lasso Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb77cc",
   "metadata": {},
   "source": [
    "### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4370c2",
   "metadata": {},
   "source": [
    "Regularized linear models, such as Lasso (L1 regularization) and Ridge (L2 regularization), offer several advantages in terms of preventing overfitting and handling multicollinearity. However, they also have limitations and may not always be the best choice for regression analysis. Here are some limitations to consider:\n",
    "\n",
    "1. Loss of Interpretability:\n",
    "The regularization terms in Lasso and Ridge push some coefficients toward zero, potentially leading to a sparse model. While this can be advantageous for feature selection, it may make the model less interpretable, especially if certain features are completely excluded.\n",
    "\n",
    "2. Sensitivity to Scaling:\n",
    "Regularized linear models are sensitive to the scale of the input features. If features have vastly different scales, the regularization term may disproportionately penalize features with larger magnitudes, impacting the model's performance. Feature scaling (normalization or standardization) is often recommended to address this issue.\n",
    "\n",
    "3. Difficulty in Handling Categorical Features:\n",
    "Lasso regularization tends to completely exclude less important features by setting their coefficients to zero. This behavior can be problematic when dealing with categorical variables, as it may exclude entire categories, leading to biased predictions. Techniques like one-hot encoding or using other regularization approaches may be necessary in such cases.\n",
    "\n",
    "4. Arbitrary Feature Selection:\n",
    "The feature selection process in Lasso is somewhat arbitrary, and small changes in the data or the model can result in different features being selected. This lack of stability in feature selection can be a drawback when the goal is to identify the most important predictors consistently.\n",
    "\n",
    "5. Limited Performance Improvement:\n",
    "Regularization techniques are most effective when there is a risk of overfitting or multicollinearity. In cases where the data is not high-dimensional or the features are not highly correlated, the benefits of regularization may be limited, and a simple linear regression model could perform equally well or better.\n",
    "\n",
    "6. Not Suitable for Non-Linear Relationships:\n",
    "Regularized linear models assume a linear relationship between the features and the target variable. If the true relationship is highly non-linear, other non-linear models or feature transformations may be more appropriate.\n",
    "\n",
    "7. Impact of Outliers:\n",
    "Lasso and Ridge regularization can be sensitive to outliers, especially when the regularization parameter is large. Outliers may disproportionately influence the penalty terms, leading to biased coefficient estimates.\n",
    "\n",
    "8. Computational Complexity:\n",
    "Solving the optimization problems associated with regularization can be computationally expensive, especially as the size of the dataset increases. This may become a limitation in scenarios where computational resources are limited.\n",
    "\n",
    "##### When to Consider Other Approaches:\n",
    "1. If interpretability is a top priority and feature selection is not crucial, a simple linear regression model might be preferred.\n",
    "2. For highly non-linear relationships, considering non-linear models or feature engineering may be more effective.\n",
    "3. When dealing with a small dataset, the risk of overfitting might be lower, and regularization may not be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33326be",
   "metadata": {},
   "source": [
    "### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f32d7",
   "metadata": {},
   "source": [
    "The choice between Model A and Model B depends on the specific goals and requirements of the problem at hand. Both RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error) are metrics used to evaluate the performance of regression models, but they capture different aspects of the prediction errors.\n",
    "\n",
    "#### Interpretation:\n",
    "\n",
    "1. RMSE (Model A): It penalizes larger errors more heavily due to the squaring operation. RMSE is suitable when larger errors have a more significant impact on the overall performance.\n",
    "2. MAE (Model B): It treats all errors equally, providing a measure of the average absolute error. MAE is suitable when all errors, regardless of magnitude, are considered equally important.\n",
    "\n",
    "#### Decision Criteria:\n",
    "\n",
    "1. Choose Model A (RMSE of 10) if:\n",
    "Larger errors have a more significant impact on the application.\n",
    "The distribution of errors is relatively normal, and there are no extreme outliers that would disproportionately affect the squared errors.\n",
    "\n",
    "2. Choose Model B (MAE of 8) if:\n",
    "All errors, regardless of size, are considered equally important.\n",
    "There are outliers or extreme values in the data, and you want the evaluation to be less sensitive to them.\n",
    "\n",
    "#### Limitations and Considerations:\n",
    "1. Scale Sensitivity: Both RMSE and MAE are scale-sensitive, meaning their values depend on the scale of the target variable. Therefore, it's important to consider the scale of the problem when interpreting these metrics.\n",
    "\n",
    "2. Application Specificity: The choice between RMSE and MAE depends on the specific application and the importance assigned to different types of errors. For example, in financial applications, larger errors may have a more significant impact on decision-making.\n",
    "\n",
    "3. Robustness to Outliers: RMSE can be sensitive to outliers due to the squaring operation, whereas MAE is more robust in the presence of outliers. If your dataset contains outliers, MAE might be a more appropriate metric.\n",
    "\n",
    "4. Model Complexity: The choice of metric may also depend on the complexity of the model and the desired balance between precision and generalization. More complex models may have a tendency to minimize RMSE, potentially leading to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f6b35",
   "metadata": {},
   "source": [
    "### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c95bf6",
   "metadata": {},
   "source": [
    "The choice between Ridge (L2 regularization) and Lasso (L1 regularization) depends on the specific characteristics of the data and the goals of the modeling task. Both regularization techniques add penalty terms to the linear regression cost function to prevent overfitting and handle multicollinearity, but they have different effects on the model's coefficients.\n",
    "\n",
    "#### Model A (Ridge Regularization with α=0.1):\n",
    "1. Ridge regularization adds a penalty term based on the sum of squared coefficients (sum_(i=1)^n θi^2).\n",
    "2. The regularization parameter (α) controls the strength of the regularization. In this case, α = 0.1.\n",
    "#### Model B (Lasso Regularization with α=0.5):\n",
    "1. Lasso regularization adds a penalty term based on the sum of absolute values of coefficients (∑ i=1n∣θi∣).\n",
    "2. The regularization parameter (α) controls the strength of the regularization. In this case, α=0.5.\n",
    "\n",
    "#### Considerations and Trade-offs:\n",
    "\n",
    "1. Sparsity and Feature Selection:\n",
    "###### Lasso (Model B): Lasso tends to produce sparser models by driving some coefficients to exactly zero. If feature selection is important, and there is a belief that some features are irrelevant, Lasso might be preferred.\n",
    "###### Ridge (Model A): Ridge does not drive coefficients to zero as aggressively as Lasso. It shrinks coefficients but typically retains all features in the model.\n",
    "\n",
    "2. Handling Multicollinearity:\n",
    "###### Ridge (Model A): Ridge is effective in handling multicollinearity by distributing the impact of correlated features. It does not eliminate any features but reduces their impact.\n",
    "###### Lasso (Model B): Lasso can also handle multicollinearity but has the tendency to select one feature among a group of highly correlated features, effectively ignoring the others.\n",
    "\n",
    "3. Impact on Coefficients:\n",
    "###### Lasso (Model B): Lasso can lead to more extreme coefficient values and drive some coefficients exactly to zero. This can be advantageous for feature selection but may make the model less interpretable.\n",
    "###### Ridge (Model A): Ridge tends to shrink coefficients towards zero but does not set them exactly to zero. It may be more suitable when maintaining interpretability is a priority.\n",
    "\n",
    "4. Choice of α:\n",
    "###### The choice of the regularization parameter (α) is crucial. Higher values of α result in stronger regularization. It's common to perform hyperparameter tuning to find the optimal α that balances bias and variance.\n",
    "\n",
    "5. Computational Complexity:\n",
    "###### Lasso tends to produce more sparse models, which can be computationally advantageous in scenarios with a large number of features.\n",
    "\n",
    "#### Decision Criteria:\n",
    "##### Choose Model A (Ridge) if:\n",
    "1. Interpretability is crucial, and you want to retain all features in the model.\n",
    "2. There is a concern about multicollinearity, and a more continuous reduction of feature impact is acceptable.\n",
    "##### Choose Model B (Lasso) if:\n",
    "1. Feature selection is a priority, and you believe that some features are irrelevant.\n",
    "2. A sparser model is preferred, and interpretability is less of a concern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
