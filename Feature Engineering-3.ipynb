{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f05ef203",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3a707",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique that transforms numerical features to a specific range, usually [0, 1]. It is achieved by subtracting the minimum value of the feature and then dividing by the range (difference between maximum and minimum values). The formula for Mi\n",
    "n-Max scaling is:\n",
    "\n",
    "Scaled Value = (Original Value −Min Value)/(Max Value−Min Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'Feature1': [2, 5, 10, 15, 20]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nDataFrame after Min-Max scaling:\")\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcc7f57",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d1d68",
   "metadata": {},
   "source": [
    "Unit Vector scaling scales each feature by dividing it by its magnitude (L2-norm or Euclidean norm), ensuring that the scaled feature vector has a length of 1. The formula for Unit Vector scaling is:\n",
    "Scaled Value=Original Value /sqrt(Sum of Squares of All Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5febf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'Feature1': [2, 5, 10, 15, 20]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Unit Vector scaling\n",
    "scaler = Normalizer(norm='l2')  # 'l2' for Euclidean norm\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nDataFrame after Unit Vector scaling:\")\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201bc454",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab09442",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the original features into a new set of uncorrelated features called principal components. These components are ordered by the amount of variance they capture, allowing for dimensionality reduction while retaining the most significant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94692c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'Feature1': [2, 5, 10, 15, 20], 'Feature2': [1, 3, 7, 12, 18]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply PCA for dimensionality reduction to 1 component\n",
    "pca = PCA(n_components=1)\n",
    "df_pca = pd.DataFrame(pca.fit_transform(df), columns=['Principal Component'])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nDataFrame after PCA:\")\n",
    "print(df_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ea1cf",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45792ebc",
   "metadata": {},
   "source": [
    "PCA is a technique for Feature Extraction, where it transforms the original features into a set of principal components. These principal components represent linear combinations of the original features and are ordered by the amount of variance they capture. By selecting a subset of principal components, you effectively perform feature extraction while retaining the most critical information in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c146524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'Feature1': [2, 5, 10, 15, 20], 'Feature2': [1, 3, 7, 12, 18]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply PCA for feature extraction to 1 component\n",
    "pca = PCA(n_components=1)\n",
    "df_pca = pd.DataFrame(pca.fit_transform(df), columns=['Principal Component'])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nDataFrame after PCA for Feature Extraction:\")\n",
    "print(df_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d4517",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b699e",
   "metadata": {},
   "source": [
    "n the context of a recommendation system for a food delivery service, Min-Max scaling can be applied to features like price, rating, and delivery time to ensure that all these features are on the same scale. Here's a step-by-step explanation:\n",
    "\n",
    "Understand the Features:\n",
    "Identify the numerical features in your dataset that need scaling. In this case, it might be features like price, rating, and delivery time.\n",
    "\n",
    "Apply Min-Max Scaling:\n",
    "Use Min-Max scaling to transform each feature to the range [0, 1]. This ensures that no single feature dominates due to its larger scale.\n",
    "Use a Min-Max scaler from a library like scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58e06d",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73134b",
   "metadata": {},
   "source": [
    "In the context of predicting stock prices with a dataset containing many features, PCA can be used for dimensionality reduction. Here's how you would apply PCA:\n",
    "\n",
    "Standardize the Data:\n",
    "Before applying PCA, it's essential to standardize the data, ensuring that all features have a mean of 0 and a standard deviation of 1. This is crucial as PCA is sensitive to the scale of the features.\n",
    "\n",
    "Apply PCA:\n",
    "Use PCA to transform the standardized dataset into principal components. These components represent linear combinations of the original features, capturing the maximum variance in the data.\n",
    "\n",
    "Determine the Number of Components:\n",
    "Analyze the cumulative explained variance ratio to determine the optimal number of principal components to retain. You may choose a threshold (e.g., 95% variance explained) to decide how many components to keep.\n",
    "\n",
    "Reduce Dimensionality:\n",
    "Retain the selected number of principal components and discard the rest. This reduces the dimensionality of the dataset while preserving the most critical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c61ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with financial and market trend features\n",
    "data = {'Feature1': [...], 'Feature2': [...], 'Feature3': [...], ...}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_standardized = scaler.fit_transform(df)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA()\n",
    "df_pca = pca.fit_transform(df_standardized)\n",
    "\n",
    "# Step 3: Determine the number of components to retain\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "# Choose the number of components that explain a desired amount of variance\n",
    "\n",
    "# Step 4: Reduce dimensionality\n",
    "selected_components = 3  # Example: Choose based on the analysis in Step 3\n",
    "df_reduced = df_pca[:, :selected_components]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb3f4a",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf32f0",
   "metadata": {},
   "source": [
    "Min-Max scaling is applied using the formula:\n",
    "Scaled Value =(Original Value −Min Value)/(Max Value−Min Value)\n",
    "\n",
    "For the given dataset, [1, 5, 10, 15, 20], the Min-Max scaling is performed as follows:\n",
    "Identify Min and Max values: Min = 1, Max = 20.\n",
    "Apply Min-Max scaling for each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b62408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = [1, 5, 10, 15, 20]\n",
    "\n",
    "# Step 1: Identify Min and Max values\n",
    "min_value = np.min(data)\n",
    "max_value = np.max(data)\n",
    "\n",
    "# Step 2: Apply Min-Max scaling\n",
    "scaled_values = [((x - min_value) / (max_value - min_value)) * 2 - 1 for x in data]\n",
    "\n",
    "print(\"Original values:\", data)\n",
    "print(\"Min-Max scaled values (-1 to 1):\", scaled_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5fcf3",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a4510c",
   "metadata": {},
   "source": [
    "In this scenario, PCA can be applied to extract principal components from the dataset. The number of principal components to retain depends on the variance explained by these components. Here's how you might proceed:\n",
    "\n",
    "Standardize the Data:\n",
    "Standardize the features to have zero mean and unit variance.\n",
    "\n",
    "Apply PCA:\n",
    "Use PCA to transform the standardized dataset into principal components.\n",
    "\n",
    "Analyze Explained Variance:\n",
    "Examine the cumulative explained variance ratio to determine how much of the total variance is captured by each principal component.\n",
    "\n",
    "Choose the Number of Components:\n",
    "Select the number of principal components that explain a sufficiently high percentage of the total variance. A common threshold is to retain components that collectively explain at least 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f768a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with features\n",
    "data = {'height': [...], 'weight': [...], 'age': [...], 'gender': [...], 'blood_pressure': [...]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_standardized = scaler.fit_transform(df)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA()\n",
    "df_pca = pca.fit_transform(df_standardized)\n",
    "\n",
    "# Step 3: Analyze explained variance\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Step 4: Choose the number of components based on explained variance\n",
    "selected_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "print(\"Number of components to retain:\", selected_components)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
