{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5835942",
   "metadata": {},
   "source": [
    "### Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23afb05f",
   "metadata": {},
   "source": [
    "Assumptions of ANOVA:\n",
    "\n",
    "Independence: Observations are independent within and across groups.\n",
    "Normality: Residuals (differences between observed and predicted values) are normally distributed for each group.\n",
    "Homogeneity of Variances (Homoscedasticity): Variances of the residuals are equal across all groups.\n",
    "Random Sampling: Data is collected through a random sampling process.\n",
    "Examples of Violations:\n",
    "\n",
    "Independence: Observations are not independent if there is a dependency or correlation between them, violating the assumption.\n",
    "Normality: If the residuals are not normally distributed, especially for small sample sizes, it can impact the accuracy of p-values and confidence intervals.\n",
    "Homogeneity of Variances: Unequal variances across groups may lead to incorrect conclusions. This is called heteroscedasticity.\n",
    "Random Sampling: If the sampling process is not random, it can introduce bias in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee4ce0",
   "metadata": {},
   "source": [
    "### Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6254bc04",
   "metadata": {},
   "source": [
    "Three types of ANOVA:\n",
    "\n",
    "One-Way ANOVA: Used when comparing means across two or more independent groups.\n",
    "Two-Way ANOVA: Extends the one-way ANOVA to two independent variables, allowing for the assessment of main effects and interaction effects.\n",
    "Repeated Measures ANOVA: Used when measurements are taken on the same group or individual at multiple time points.\n",
    "Situations for Each:\n",
    "\n",
    "One-Way ANOVA: Used when comparing means across multiple groups (e.g., comparing average scores of students from different schools).\n",
    "Two-Way ANOVA: Used when there are two independent variables and you want to examine their main effects and interaction effects (e.g., analyzing the impact of both diet and exercise on weight loss).\n",
    "Repeated Measures ANOVA: Used when the same subjects are used for each treatment (e.g., measuring the effect of a drug at different time points)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c0807",
   "metadata": {},
   "source": [
    "### Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee108dd7",
   "metadata": {},
   "source": [
    "Partitioning of Variance in ANOVA:\n",
    "In ANOVA, the total variance in the data is partitioned into different sources:\n",
    "\n",
    "Total Sum of Squares (SST): Variability of the dependent variable across all observations.\n",
    "Explained Sum of Squares (SSE): Variability explained by the model or treatment effect.\n",
    "Residual Sum of Squares (SSR): Unexplained or residual variability.\n",
    "Understanding this partitioning is crucial as it helps identify how much of the total variability is explained by the model (treatment) and how much is due to random variation (residuals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aac80c",
   "metadata": {},
   "source": [
    "### Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f9c5dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST =  283.3333333333333\n",
      "SSE =  120.0\n",
      "SSR =  163.33333333333331\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "# Example data for three groups\n",
    "group1 = [23, 25, 27, 29, 31]\n",
    "group2 = [18, 20, 22, 24, 26]\n",
    "group3 = [15, 17, 19, 21, 23]\n",
    "\n",
    "# Combine data\n",
    "all_data = group1 + group2 + group3\n",
    "\n",
    "# Calculate means\n",
    "overall_mean = np.mean(all_data)\n",
    "group_means = [np.mean(group) for group in [group1, group2, group3]]\n",
    "\n",
    "# Calculate SST\n",
    "SST = np.sum((all_data - overall_mean)**2)\n",
    "\n",
    "# Calculate SSE\n",
    "SSE = np.sum([(x - mean)**2 for group, mean in zip([group1, group2, group3], group_means) for x in group])\n",
    "\n",
    "# Calculate SSR\n",
    "SSR = SST - SSE\n",
    "\n",
    "print('SST = ',SST)\n",
    "print('SSE = ',SSE)\n",
    "print('SSR = ',SSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f3e1f",
   "metadata": {},
   "source": [
    "### Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f40387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data for two-way ANOVA\n",
    "# Assuming 'factor1' and 'factor2' are the two independent variables\n",
    "# 'response' is the dependent variable\n",
    "model = ols('response ~ factor1 * factor2', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Extract main effects and interaction effects\n",
    "main_effect_factor1 = anova_table['sum_sq']['factor1']\n",
    "main_effect_factor2 = anova_table['sum_sq']['factor2']\n",
    "interaction_effect = anova_table['sum_sq']['factor1:factor2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785be45c",
   "metadata": {},
   "source": [
    "### Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8da3b1",
   "metadata": {},
   "source": [
    "If F-statistic is 5.23 and p-value is 0.02:\n",
    "\n",
    "F-statistic: Indicates the ratio of variance between groups to variance within groups.\n",
    "p-value: Probability of obtaining such results if the null hypothesis (no group differences) is true.\n",
    "Interpretation:\n",
    "\n",
    "Since the p-value (0.02) is less than the significance level (e.g., 0.05), we reject the null hypothesis.\n",
    "There is enough evidence to suggest that there are significant differences between at least two groups.\n",
    "However, the specific groups with significant differences would need further post-hoc tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aadadd5",
   "metadata": {},
   "source": [
    "### Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f038da",
   "metadata": {},
   "source": [
    "Handling Missing Data in Repeated Measures ANOVA:\n",
    "1. Complete Case Analysis (Listwise Deletion):\n",
    "Method: Exclude cases with missing data on any variable involved in the analysis.\n",
    "Consequences: Reduces sample size and may lead to biased results if the missing data are not missing completely at random (MCAR). Can result in loss of statistical power.\n",
    "2. Imputation Methods:\n",
    "Mean Imputation: Replace missing values with the mean of the observed values for that variable.\n",
    "Last Observation Carried Forward (LOCF): Use the last observed value for a missing data point.\n",
    "Linear Interpolation: Estimate missing values based on the trend between observed values.\n",
    "Consequences: Imputation methods assume that missing data can be predicted or imputed based on observed data. May introduce bias if the assumption is violated. Reduces variability but may lead to more accurate estimation if data are missing at random (MAR).\n",
    "3. Multiple Imputation:\n",
    "Method: Generate multiple complete datasets with different imputed values for missing data and analyze each dataset separately.\n",
    "Consequences: Preserves variability and accounts for uncertainty due to missing data. Generally, considered more robust, but computationally more intensive.\n",
    "Potential Consequences of Different Methods:\n",
    "1. Bias: Using imputation methods may introduce bias if the missing data are not missing completely at random (MCAR) or missing at random (MAR). Complete case analysis may also lead to biased results if the assumption is violated.\n",
    "2. Precision and Power: Complete case analysis reduces sample size, leading to less precise estimates and reduced statistical power. Imputation methods may improve precision but may not fully recover the lost power.\n",
    "3. Assumption Violation: Imputation methods assume a specific pattern for missing data. If this assumption is violated, imputation results may be biased.\n",
    "4. Multiple Comparisons: With multiple imputations, there's the potential for multiple comparisons, and researchers should carefully consider the adjusted significance levels.\n",
    "\n",
    "Recommendation:\n",
    "Choose the method based on the characteristics of the missing data and the assumptions being met.\n",
    "Sensitivity analyses, such as comparing results with and without imputation, can provide insights into the robustness of the findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964535c3",
   "metadata": {},
   "source": [
    "### Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f626e5",
   "metadata": {},
   "source": [
    "After conducting an Analysis of Variance (ANOVA) and finding a significant difference among group means, post-hoc tests are often employed to explore pairwise comparisons between groups. Common post-hoc tests include:\n",
    "1. Tukey's Honestly Significant Difference (HSD):\n",
    "Use Case: Tukey's HSD is conservative and suitable when there are equal sample sizes across groups. It controls the familywise error rate, making it suitable for situations where multiple pairwise comparisons are performed.\n",
    "Example: In a study comparing the mean scores of three different teaching methods, Tukey's HSD can be used to identify specific pairs of methods that differ significantly.\n",
    "2. Bonferroni Correction:\n",
    "Use Case: Bonferroni is a conservative correction for multiple comparisons and is suitable when maintaining a low overall Type I error rate is crucial.\n",
    "Example: Suppose you are conducting multiple pairwise comparisons between the means of different drug treatments. The Bonferroni correction would be useful to control for the increased risk of making a Type I error due to multiple tests.\n",
    "3. Sidak Correction:\n",
    "Use Case: Similar to Bonferroni but less conservative. Useful when a moderate level of Type I error control is desired.\n",
    "Example: If you are comparing the mean scores of several treatment groups and want to control the overall risk of Type I error, Sidak correction may be an appropriate choice.\n",
    "4. Duncan's Multiple Range Test:\n",
    "Use Case: Duncan's test is less conservative than Tukey's HSD and is suitable when the assumption of equal variances is met.\n",
    "Example: In an agricultural study comparing the yield of different fertilizers, Duncan's test can help identify specific pairs of fertilizers with significantly different yields.\n",
    "5. Scheffe's Test:\n",
    "Use Case: Scheffe's test is more conservative but has the advantage of not assuming equal variances or sample sizes.\n",
    "Example: When comparing the performance of various machine learning algorithms across different datasets, Scheffe's test may be used for pairwise comparisons.\n",
    "Example Scenario\n",
    "Consider a study examining the effectiveness of three different teaching methods (A, B, C) on student performance. After conducting a one-way ANOVA, if the ANOVA indicates a significant difference in mean scores, you may use post-hoc tests to identify specific pairs of teaching methods that differ significantly. For instance, Tukey's HSD can be applied to determine which pairs of teaching methods have significantly different mean scores, providing more detailed insights than the omnibus ANOVA results.\n",
    "\n",
    "Post-hoc tests are essential when dealing with multiple group comparisons after ANOVA to identify specific differences among group means and avoid making Type I errors due to multiple testing. The choice of post-hoc test depends on factors such as assumptions about variances and sample sizes, as well as the desired level of Type I error control."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
